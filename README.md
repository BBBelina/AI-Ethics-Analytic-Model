# AI-Ethics-Analytic-Model

## Project Overview

This project aims to evaluate the policies related to Large Language Models (LLMs) developed by OpenAI and Google. We assess whether these policies align with a set of pre-defined Policy Compliance Indicators.
We use a combination of Automated Document Analysis, Manual Sampling Verification, and Background Research to ensure a comprehensive and reliable assessment.

## ğŸ” Methodology
1. Automated Document Analysis

The policies of both companies are analyzed using automated text processing, including document chunking, keyword matching, and comparison with predefined indicators.

2. Human Sampling Verification

To ensure the accuracy and consistency of the automated output, we conduct manual sampling to verify the results.

3. Background Research

We perform background research and cross-check policy content with publicly available sources to ensure a thorough and objective evaluation.

## ğŸ“ Project Structure

project/  
â”‚  
â”œâ”€â”€ data/  
â”‚   â”œâ”€â”€ google_document_scores.csv     
â”‚   â”œâ”€â”€ google_segmentation.csv      
â”‚   â”œâ”€â”€ openai_document_scores.csv   
â”‚   â””â”€â”€ openai_segmentation.csv     
â”‚  
â”œâ”€â”€ scripts/  
â”‚   â”œâ”€â”€ add.py                      
â”‚   â”œâ”€â”€ scraping.py                  
â”‚   â”œâ”€â”€ visualization_google.ipynb  
â”‚   â””â”€â”€ visualization_openai.ipynb   
â”‚  
â””â”€â”€ README.md  



## ğŸ“š Reference Materials

For more detailed reference documents, please visit:
https://drive.google.com/drive/u/1/folders/1qAuQaz2P6zvHn1UttUM1K4M5PtElpwmk

## ğŸ“Š Outputs

The project outputs include:
The final compliance scores for OpenAI and Google, based on the selected indicators.
Statistical analysis of the policies across various dimensions.
Visualizations showing the differences and overall trends across the policies.
